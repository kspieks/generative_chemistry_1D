# Pretrained ChEMBL Model
Model comes from [https://github.com/maxime-langevin/scaffold-constrained-generation/tree/master/data/DistributionLearningBenchmark](https://github.com/maxime-langevin/scaffold-constrained-generation/tree/master/data/DistributionLearningBenchmark).

REINVENT originally used a regular expression that could only parse individual characters ([link](https://github.com/MarcusOlivecrona/REINVENT/blob/master/data_structs.py#L42)) so Br was represented by R and Cl was represented by L. Because R and L are not actual elements, we had to always transform the generated SMILES string by converting those tokens afterwards using something like `smi.replace("L", "Cl").replace("R", "Br")`. I found this slightly annoying, especially because it could have been addressed during the initial tokenization stage. However, because REINVENT originally sorted all vocab tokens, this meant that we can't just change R to Br and L to Cl in the vocab.txt file because upon sorting, the order of the tokens would be all jumbled i.e., it will mess up the mapping that converts generated indices to tokens whose sequence ultimately form a SMILES string. So the solution implemented in this repo is to not sort the tokens within the `Vocabulary` class so that the order of tokens from the vocab.txt file is preserved. This allows us to change R to Br and L to Cl within this vocab file once and no longer worry about converting every generated SMILES string.
